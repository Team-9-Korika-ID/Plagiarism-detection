{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf0c36f99a0451590da4b4c3fb161fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f17d753b3d442d295c5862c0269305c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fce003efadf42a4a550c47727eca252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204358c9bbe5429eb686e4d8ca748da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ffb02830e9b4ac3bdb7593b773d6aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec3e0524d6f4e3ea1a9bbd6134fe43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skor Kesamaan: 0.8118517994880676\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Memuat model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Mendefinisikan dua kalimat\n",
    "kalimat1 = \"Ini adalah contoh kalimat pertama.\"\n",
    "kalimat2 = \"Ini adalah contoh kalimat kedua.\"\n",
    "\n",
    "# Menghitung embeddings untuk kedua kalimat\n",
    "embeddings1 = model.encode(kalimat1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(kalimat2, convert_to_tensor=True)\n",
    "\n",
    "# Menghitung kesamaan kosinus antara dua embeddings\n",
    "kesamaan = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "print(f\"Skor Kesamaan: {kesamaan.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skor Kesamaan antara kalimat 1 dan 2: 1.0000001192092896\n",
      "Skor Kesamaan antara kalimat 1 dan 3: 0.8827536702156067\n",
      "Skor Kesamaan antara kalimat 2 dan 3: 0.8827536702156067\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Memuat model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Dataframe\n",
    "data_lite = {'Nama': ['Andi', 'Budi', 'Anton'],\n",
    "             'Jawaban': ['Revolusi Industri 4.0 merupakan periode transformasi industri yang ditandai oleh integrasi teknologi digital ke dalam proses produksi.',\n",
    "                         'Revolusi Industri 4.0 merupakan periode transformasi industri yang ditandai oleh integrasi teknologi digital ke dalam proses produksi.',\n",
    "                         'Revolusi Industri 4.0 merupakan upaya transformasi menuju perbaikan dengan mengintegrasikan dunia online dan lini produksi di industri']}\n",
    "df = pd.DataFrame(data_lite)\n",
    "\n",
    "# Mendefinisikan kalimat\n",
    "kalimat = df['Jawaban']\n",
    "\n",
    "# Menghitung embeddings untuk kalimat\n",
    "embeddings = model.encode(kalimat.tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Menghitung kesamaan kosinus antara setiap pasangan kalimat\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i + 1, len(embeddings)):\n",
    "        kesamaan = util.pytorch_cos_sim(embeddings[i], embeddings[j])\n",
    "        print(f\"Skor Kesamaan antara kalimat {i+1} dan {j+1}: {kesamaan.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrix:\n",
      "Nama       Andi      Budi     Anton\n",
      "Nama                               \n",
      "Andi   0.000000  1.000000  0.882754\n",
      "Budi   1.000000  0.000000  0.882754\n",
      "Anton  0.882754  0.882754  0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Memuat model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Dataframe\n",
    "data_lite = {'Nama': ['Andi', 'Budi', 'Anton'],\n",
    "             'Jawaban': ['Revolusi Industri 4.0 merupakan periode transformasi industri yang ditandai oleh integrasi teknologi digital ke dalam proses produksi.',\n",
    "                         'Revolusi Industri 4.0 merupakan periode transformasi industri yang ditandai oleh integrasi teknologi digital ke dalam proses produksi.',\n",
    "                         'Revolusi Industri 4.0 merupakan upaya transformasi menuju perbaikan dengan mengintegrasikan dunia online dan lini produksi di industri']}\n",
    "df = pd.DataFrame(data_lite)\n",
    "\n",
    "# Mendefinisikan kalimat\n",
    "kalimat = df['Jawaban']\n",
    "\n",
    "# Menghitung embeddings untuk kalimat\n",
    "embeddings = model.encode(kalimat.tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Inisialisasi matriks similarity dengan nol\n",
    "num_sentences = len(kalimat)\n",
    "similarity_matrix = np.zeros((num_sentences, num_sentences))\n",
    "\n",
    "# Mengisi matriks similarity\n",
    "for i in range(num_sentences):\n",
    "    for j in range(i + 1, num_sentences):\n",
    "        similarity_matrix[i][j] = util.pytorch_cos_sim(embeddings[i], embeddings[j])\n",
    "\n",
    "# Mengisi matriks dengan simetri diagonal\n",
    "for i in range(num_sentences):\n",
    "    for j in range(0, i):\n",
    "        similarity_matrix[i][j] = similarity_matrix[j][i]\n",
    "\n",
    "# Membuat dataframe dari matriks similarity\n",
    "similarity_df = pd.DataFrame(similarity_matrix, columns=df['Nama'], index=df['Nama'])\n",
    "\n",
    "print(\"Similarity Matrix:\")\n",
    "print(similarity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "def preprocess_data(data_path, sample_size):\n",
    " \n",
    "  # Read the data from specific path\n",
    "  data = pd.read_csv(data_path, low_memory=False)\n",
    "\n",
    "  # Drop articles without Abstract\n",
    "  data = data.dropna(subset = ['abstract']).reset_index(drop = True)\n",
    "\n",
    "  # Get \"sample_size\" random articles\n",
    "  data = data.sample(sample_size)[['abstract']]\n",
    " \n",
    " return data\n",
    " \n",
    "# Read data & preprocess it\n",
    "data_path = \"./data/cord19_source_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer,  AutoModelForSequenceClassification\n",
    " \n",
    "# Load bert model\n",
    "model_path = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path,\n",
    "                                         do_lower_case=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,\n",
    "                                                         output_attentions=False,\n",
    "                                                         output_hidden_states=True)\n",
    "                                                        \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    " \n",
    "def create_vector_from_text(tokenizer, model, text, MAX_LEN = 510):\n",
    "  \n",
    "    input_ids = tokenizer.encode(\n",
    "                        text,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = MAX_LEN,                          \n",
    "    )   \n",
    "    results = pad_sequences([input_ids], maxlen=MAX_LEN, dtype=\"long\",\n",
    "                              truncating=\"post\", padding=\"post\")\n",
    "    # Remove the outer list.\n",
    "    input_ids = results[0]\n",
    "    # Create attention masks   \n",
    "    attention_mask = [int(i>0) for i in input_ids]\n",
    "    # Convert to tensors.\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    # Add an extra dimension for the \"batch\" (even though there is only one\n",
    "    # input in this batch.)\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    attention_mask = attention_mask.unsqueeze(0)\n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "    # Run the text through BERT, and collect all of the hidden states produced\n",
    "    # from all 12 layers.\n",
    "    with torch.no_grad():       \n",
    "        logits, encoded_layers = model(\n",
    "                                    input_ids = input_ids,\n",
    "                                    token_type_ids = None,\n",
    "                                    attention_mask = attention_mask,\n",
    "                                    return_dict=False)\n",
    "\n",
    "    layer_i = 12 # The last BERT layer before the classifier.\n",
    "    batch_i = 0 # Only one input in the batch.\n",
    "    token_i = 0 # The first token, corresponding to [CLS]\n",
    "      \n",
    "    # Extract the vector.\n",
    "    vector = encoded_layers[layer_i][batch_i][token_i]\n",
    "    # Move to the CPU and convert to numpy ndarray.\n",
    "    vector = vector.detach().cpu().numpy()\n",
    "    return(vector)\n",
    " \n",
    "def create_vector_index(data):\n",
    "  \n",
    "   # The list of all the vectors\n",
    "   vectors = []\n",
    "  \n",
    "   # Get overall text data\n",
    "   source_data = data.abstract.values\n",
    "  \n",
    "   # Loop over all the comment and get the embeddings\n",
    "   for text in tqdm(source_data):\n",
    "      \n",
    "       # Get the embedding\n",
    "       vector = create_vector_from_text(tokenizer, model, text)\n",
    "      \n",
    "       #add it to the list\n",
    "       vectors.append(vector)\n",
    "  \n",
    "   data[\"vectors\"] = vectors\n",
    "   data[\"vectors\"] = data[\"vectors\"].apply(lambda emb: np.array(emb))\n",
    "   data[\"vectors\"] = data[\"vectors\"].apply(lambda emb: emb.reshape(1, -1))\n",
    "   return data\n",
    "# Create the vector index\n",
    "vector_index = create_vector_index(source_data)\n",
    "vector_index.sample(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
